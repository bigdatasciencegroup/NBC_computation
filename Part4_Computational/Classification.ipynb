{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`This lab was written by Charles Frye, a Neuroscience PhD student in the DeWeese Lab, and edited by Sara Popham, a Neuroscience PhD student in the Gallant Lab, for use in the Neuroscience Boot Camp.`\n",
    "\n",
    "`This is also part of the materials for the Neur299 course (Applied Statistics for Neuroscience) that is offered every spring.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "import util.utils as utils\n",
    "import util.shared as shared\n",
    "\n",
    "shared.format_plots()\n",
    "shared.format_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab, we used a regression model to predict brain responses when the subject saw a number of different objects (e.g. faces, places, and 1703 other objects and actions). These models connected two variables that can take on any of an infinite number of values. These are called *continuous* variables, and models that use a continuous variable to predict a continuous variable are called *regression* models.\n",
    "\n",
    "Frequently, however, the variable we want to predict only takes on one of two or a small number of values: did the mouse get the disease, or did it not? did the neuron fire an action potential, or did it not? was the weather sunny, cloudy, rainy, or extreme? and so on. These are called *discrete* variables, and models that use a continuous variable to predict a discrete variable are called *classification* models or *classifiers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classification problem, we are given pairs of observations and labels and we want to come up with a \"classifier\", a function that can\n",
    "\n",
    "1. correctly assign as many of the observed labels to the observed values as possible\n",
    "1. given new observations, predict the correct label\n",
    "\n",
    "It's useful to think of our classifier as solving two problems: first, it calculates the probability that the given point would have each label. Then, it uses some decision rule to choose a label based on the relative probabilities. \n",
    "\n",
    "If our goal is to maximize the probability that our prediction is correct, then the decision rule is easy: simply pick the label with the largest probability.\n",
    "\n",
    "So the meat of our classifier is in how it solves the first problem: calculating the conditional probability of each label given the provided data point.\n",
    "\n",
    "How do we pick the best classifier? We can make headway by using the principle of maximum likelihood: when given the choice between a bunch of classifiers, we pick the classifier that maximizes the likelihood function of the data.\n",
    "\n",
    "$$\n",
    "    \\text{best classifier} = \\arg\\!\\max_\\text{classifiers} \\prod_\\text{dataset} p_\\text{classifier}(\\text{true label}\\ |\\ \\text{observation})\n",
    "$$\n",
    "\n",
    "Here $p_\\text{classifier}$ means \"the probability distribution the classifier uses to make its decision\". As we change our classifier, this distribution will change. We're looking for the one that maximizes the likelihood.\n",
    "\n",
    "While this expression is intuitive, it's unfortunately not usable by itself -- how do we relate the form of a classifier to $p_{\\text{classifier}}$? how do we maximize over \"classifiers\"? In the following, we'll rewrite this into a more usable form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bernoulli Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we need to figure out how to write down a probability distribution for our label variable. We need to do a bit of work here because we've primarily considered distributions for continuous variables.\n",
    "\n",
    "In order to simplify the math, let's assume that there are only two labels. These labels could be \"sick\" and \"healthy\" or \"active neuron\" and \"inactive neuron\" or, more generally, \"class A\" or \"class B\". Because it only has two values, we call our label a *binary* variable.\n",
    "\n",
    "It's easy to write down the probability distribution for a binary variable. There's only one parameter, indicated, as is tradition, by $\\theta$, which tells us the probability of getting one of the outcomes. That's all we need, since then the probability of the other outcome is just $1-\\theta$.\n",
    "\n",
    "$$\n",
    "p(\\text{label A}\\ \\lvert\\ \\theta) = \\theta \\\\\n",
    "p(\\text{label B}\\ \\lvert\\ \\theta) = 1-\\theta\n",
    "$$\n",
    "\n",
    "This kind of a probability distribution is called a *Bernoulli distribution*, after [Jacob Bernoulli](https://en.wikipedia.org/wiki/Jacob_Bernoulli), a 17th century Swiss physicist and mathematician who wrote one of the foundational works in probability, [*Ars Conjectandi*](https://en.wikipedia.org/wiki/Ars_Conjectandi). It's the probability distribution of a coin flip (using the labels \"heads\" and tails\"), with parameter $\\theta=0.5$ if the coin is fair.\n",
    "\n",
    "In order to make our labels work with our math, we need to turn our labels into numbers. It will turn out to be convenient to choose the numeric labels \"0\" and \"1\". This lets us cleverly rewrite the Bernoulli distribution above as\n",
    "\n",
    "$$\n",
    "p(\\text{label}\\lvert\\theta) = \\theta^{\\text{label}}(1-\\theta)^{1-\\text{label}}\n",
    "$$\n",
    "\n",
    "There's nothing here that wasn't in the original description above. We've simply rewritten it in a form that lets us talk about $p(Y\\lvert\\theta)$ as a single function, instead of one function for $Y=1$ and another for $Y=0$. We achieved this by multiplying the two original functions by each other and then adding in exponents so that each term is 1 when it's not being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Maximum Likelihood\n",
    "\n",
    "Let's plug the Bernoulli distribution into our maximum likelihood expression, then convert to the \"minimum surprise\" from by taking the negative log of the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    \\text{best classifier} &= \\arg\\!\\max_\\text{classifiers} \\prod_\\text{dataset} p_\\text{classifier}(\\text{true label}\\ |\\ \\text{observation}) \\\\\n",
    "     &= \\arg\\!\\max_\\text{classifiers} \\prod_\\text{dataset}\n",
    "                 p_\\text{classifier}(\\text{label} = 1\\ |\\ \\text{observation})^\\text{true label}\n",
    "                 p_\\text{classifier}(\\text{label} = 0\\ |\\ \\text{observation})^{1-\\text{true label}} \\\\\n",
    "     &= \\arg\\!\\min_\\text{classifiers} -\\sum_\\text{dataset}\n",
    "                 \\log \\left(p_\\text{classifier}(\\text{label} = 1\\ |\\ \\text{observation})^\\text{true label}\n",
    "                 p_\\text{classifier}(\\text{label} = 0\\ |\\ \\text{observation})^{1-\\text{true label}}\\right) \\\\\n",
    "     &= \\arg\\!\\min_\\text{classifiers} \\sum_\\text{dataset} \n",
    "                     \\text{true label}*-\\log p_\\text{classifier}(\\text{label} = 1\\ |\\ \\text{observation}) \\\\\n",
    "                & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n",
    "                +(1-\\text{true label})*-\\log p_\\text{classifier}(\\text{label} = 0\\ |\\ \\text{observation}) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks complicated, but it's not once you break it down, term by term and line by line. \n",
    "\n",
    "First, notice that the true label for a data point is always $0$ or $1$, so only one of the two terms inside the sum will be non-zero. \n",
    "\n",
    "Let's focus on the first term. It says that, when the true label is $1$, the thing we want to minimize is the negative log probability under the model, or model's *surprise*, that the provided observation has label $1$. The second term says to reduce the surprise that the observation has the label $0$, but only when the true label is $0$. This is similar to how we fit our regression models, where our goal was also to be minimally surprised by the data we saw.\n",
    "\n",
    "One last trick will let us write the whole thing in terms of the probability that the label is $1$. Noting that the probability that the label is $0$ is just 1 minus this value, we write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    \\text{best classifier}&= \\arg\\!\\min_\\text{classifiers} \\sum_\\text{dataset} \n",
    "                \\text{true label}*-\\log p_\\text{classifier}(\\text{label} = 1\\ |\\ \\text{observation}) \\\\\n",
    "                 & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n",
    "                 +(1-\\text{true label})*\n",
    "                 -\\log\\left( 1 - p_\\text{classifier}(\\text{label} = 1\\ |\\ \\text{observation})\\right) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lets us simplify the main component of our classifier, the part that calculates the conditional probability distribution of the labels. We can think of it as just calculating the probability that the label is $1$. This is just a single-output function of the observation, so we can re-write the above as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{best classifier}&= \\arg\\!\\min_{\\text{functions } f} \\sum_\\text{dataset} \n",
    "                \\text{true label}*-\\log f(\\text{observation})\n",
    "                 +(1-\\text{true label})*-\\log\\left( 1- f(\\text{observation})\\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To save space, let's introduce the symbols $X$ for the observation and $Y$ for the label:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{best classifier}&= \\arg\\!\\min_{\\text{functions } f} \\sum_\\text{dataset} \n",
    "                Y*-\\log f(X)\n",
    "                 +(1-Y)*-\\log\\left( 1- f(X)\\right) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in designing classifiers will now be to pick families of functions to optimize the above expression over. Perhaps linear, linearized, and nonlinear functions?\n",
    "\n",
    "Unfortunately, we can't just plow ahead with same families we used for regression. Our functions are already very special: they represent conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to directly fit a linear model to a classification dataset, disaster can strike. Below, we load the classic `iris` dataset and fit a linear model that tries to predict the binary variable indicating whether a given flower is an *Iris setosa* based on the width of its sepal leaf. This was [one of the first datasets used for classification](https://en.wikipedia.org/wiki/Iris_flower_data_set), originally by [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sns.load_dataset(\"iris\")\n",
    "\n",
    "data[\"is setosa?\"] = data[\"species\"] == \"setosa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(data=data, x=\"sepal_width\", y=\"is setosa?\", logistic=False, ci=95);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 Why is this linear model a bad model for the conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 What would happen to the fitted line if we were to observe another point, this one with a `sepal_width` of 5 that was a *setosa*? What if we observed 100 points like that? Why is this bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the Logistic Function\n",
    "\n",
    "This is a shame. Our use of parametric families of functions was huge for expanding our regression models. It let us transform optimizations over functions (which are defined at an infinite number of points!) into optimizations over parameters, which we get to pick the number of. \n",
    "\n",
    "Our solution is to split $f$ into two parts: one function, $\\phi$, that can take on any form, and another function, $\\sigma$, that takes functions of any form and turns them into valid conditional probabilities. That is, $\\sigma$ is a function that takes any number and returns a number between 0 and 1.\n",
    "\n",
    "The function we use is called the *logistic* function. Mathematically, it looks like this:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+\\mathrm{e}^{-x}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a graph, it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function is used because it connects the *log-odds ratio* that each class would produce the observation to the conditional probability of a class given the observation. For more, see the addendum at the end of the lab.\n",
    "\n",
    "In this form, our classifier problem is written:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    \\text{best classifier} &= \\arg\\!\\min_{\\text{functions } \\phi} \\sum_{dataset} \n",
    "                Y*-\\log \\sigma(\\phi(X))\n",
    "                 +(1-Y)*-\\log\\left( 1- \\sigma(\\phi(X))\\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can now pick our favorite parametric function family and use it, without issue, as the core component of a classifier.\n",
    "\n",
    "We now turn to examining the three types of classifiers derived from our three function families, and end by considering classifiers that fall outside of the logistic schema, nonparametric classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear classifer is a classifier that uses a linear function $\\phi$. Linear functions can be written as dot products, so we can write\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{best linear classifier} &= \\arg\\!\\min_{\\phi} \\sum_{dataset} \n",
    "                Y*-\\log \\sigma(\\phi^T X)\n",
    "                 +(1-Y)*-\\log\\left( 1- \\sigma(\\phi^T X)\\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finding a linear classifier is also called *logistic regression*.\n",
    "\n",
    "Whenever $\\phi^T X$ is greater than $0$, we predict that $X$ has label 1, because then $\\sigma(0)$ is greater than $0.5$. Remember that the output of $\\sigma$ is the conditional probability that $X$ has label 1. Similarly, we predict that $X$ has label 0 whenever $\\sigma$ is less than 0.\n",
    "\n",
    "The set of points $D$ where $\\phi^T D$ is exactly 0 are called the *decision boundary* of our model. This is the set of all vectors that are orthogonal to the vector $\\phi$. Therefore the decision boundary is a point in one dimension, a line in two dimensions, or a plane in three dimensions. Since we usually augment our data with a constant vector, this point/line/plane doesn't need to go through the origin.\n",
    "\n",
    "Notice that the dimension of each boundary is one dimension less than the dimension of the space. The generalization of this idea to dimensions greater than 3 is the *hyperplane*. Notice also that each boundary divides the space into only two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's apply logistic regression to the dataset from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(data=data, x=\"sepal_width\", y=\"is setosa?\", logistic=True, ci=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 How does it perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the situation that caused an issue for our linear regression model: we observe many points that have label `1` and a `sepal_width` of `5`.\n",
    "\n",
    "#### Q4 How would the linear classifier's behavior be different (better) than the linear regression model's behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearized Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it's not particularly common, it's certainly possible to make a linearized classifer. As before, we introduce a nonlinear function $f$ that transforms or augments $X$ before the parameters, $\\phi$, do a linear transformation of this output. For example, in a polynomial linearized model, $f$ would be a function that returns the input, the square of the input, the cube of the input, and so on.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{best linear classifier} &= \\arg\\!\\min_{\\phi} \\sum_{dataset} \n",
    "                Y*-\\log \\sigma(\\phi^T f(X))\n",
    "                 +(1-Y)*-\\log\\left( 1- \\sigma(\\phi^T f(X))\\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "However, the use cases for linearized classifiers are pretty limited. Linearized models rely on strong hypotheses about the relationship between the input variables and the outputs, which are often harder to formulate in the setting of classification than in regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric Nonlinear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, when folks want to perform classification without assuming a linear model, they use flexible parametric nonlinear classifiers. If we call the parameters $\\theta$, we can write our classifier optimization problem as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{best parametric nonlinear classifier} &= \\arg\\!\\min_{\\theta} \\sum_{dataset} \n",
    "                Y*-\\log \\sigma(\\phi(X,\\theta))\n",
    "                 +(1-Y)*-\\log\\left( 1- \\sigma(\\phi(X,\\theta))\\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Frequently, these classifiers have stacked linear-nonlinear transformations, with the parameters only modifiying the linear transformation. These are called *neural networks*. The decision boundaries of a neural network are much more complex than simple hyperplanes, so they can separate labels that are heavily intertwined in the data space. If you'd like to see some neural networks in action tackling simple but difficult classification problems, check out Google's [Neural Network Playground](http://playground.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Parametric Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do classification in a non-parametric way. \n",
    "\n",
    "The most common approach is called *k-nearest-neighbors*. In a nearest-neighbors approach, we drop the idea of using maximum likelihood and just adopt the heuristic that points in the same class are more likely to be closer to each other. To predict the class of a new, unlabeled datapoint, we simply look around at the *k* closest points in the dataset and take a \"majority vote\" on what the class label should be.\n",
    "\n",
    "This approach is powerful and flexible in that, given enough data, it can perform well when classifying data from absolutely any distribution. It's not without its drawbacks, however. Because it's nonparametric, we expect that it will have a tendency to overfit the data and be unreliable unless the amount of data provided is large. Also, nonparametric models require you to keep the entire dataset around, which can be prohibitive when that dataset is, say, an image of very face you've ever seen in your life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did for regression, we can visualize the implicit models of our classifiers. As before, we look at the output of our classifier before it makes the classification decision, i.e. after $\\sigma(\\phi(X))$, as a function of $X$. \n",
    "\n",
    "We plot the model output as a surface -- the input dimensions on the x and y axes and the output on the z axis. The z height is also indicated using color. The scale runs from red at 0 to blue at 1, with white at 0.5. Red thus indicates a high probability of label 0 and blue a high probability of label 1.\n",
    "\n",
    "We also visualize, with a scatter plot, a random dataset generated according to the model's conditional distribution. Inputs are again plotted on the x and y axes and label on the z axis. A black dot has label 1 and a white dot label 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we look at a linear classifier. The function `plotLinearModel` will generate a random linear classifier, then plot it as described above. If you want to see a particular linear classifier, pass a specific set of weights, as indicated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "utils.plot_linear_model()\n",
    "#utils.plot_linear_model(weights = np.asarray([[5., 2., 1.25]]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6 How do you determine the decision boundaries of a model using this chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7 What do the decision boundaries for linear classifiers look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearized Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we consider linearized classifiers: models that apply a linear $\\phi$ to multiple fixed non-linear transformations of its input.\n",
    "\n",
    "Three linearized models with different weights and non-linearities are defined by `generate_linearized_models` and stored in the dictionary `phi_f`. Their keys are `quadratic`, `trig` and `checker`. You can adjust which model is plotted by changing the value of `model` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_f = utils.generate_linearized_models()\n",
    "\n",
    "model = 'trig'\n",
    "\n",
    "utils.plot_linearized_model(phi_f[model], extent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out each model: `quadratic`, `trig`, and `checker`.\n",
    "\n",
    "#### Q8 What do the decision boundaries look like for each model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Fitting and Model Accuracy for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we can't find a closed-form solution for the classifier that minimizes our cost function. Instead, we have to use gradient descent, even in the linear case.\n",
    "\n",
    "The usual metric for accuracy is percent correctly classified. If this metric is computed on the sample it is biased upwards and would favor more complex models that can be overfit. This is why normally we instead look at the metric for a cross-validated sample, because even though it is biased downwards, it doesn't favor any particular model.\n",
    "\n",
    "To perform gradient descent, we'll use [scikit-learn](http://scikit-learn.org/stable/). Scikit-learn is the premier Python toolbox for implementing established algorithms for classification, regression, and many other machine learning problems.\n",
    "\n",
    "The cell below (adapted from [an example](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py) in the [`sklearn` documentation](http://scikit-learn.org/stable/documentation.html)) will run up to three classifiers: a linear classifier (`SVC`), a neural network classifier (`MLPClassifier`), and a nearest-neighbors classifier (`KNeighborsClassifier`). These are provided via the list `classifiers`. Keep them in the same order, commenting out whichever you're not currently using.\n",
    "\n",
    "The details of the figure are explained below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "N = 500\n",
    "holdout = .2\n",
    "\n",
    "# hyperparameter for nearest-neighbors\n",
    "k = 1\n",
    "\n",
    "# hyperparameter for neural net\n",
    "hidden_layer_sizes = [10,5,2]\n",
    "\n",
    "classifiers = [\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "\n",
    "    MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                  alpha=0.05,max_iter=2500,\n",
    "                 learning_rate = \"invscaling\"),\n",
    "    \n",
    "    KNeighborsClassifier(k),\n",
    "]\n",
    "\n",
    "utils.run_classifiers(classifiers,N=N,holdout=holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left column of the figure shows the three datasets. Each dataset has two dimensional input, and the label is indicated by color. The size of the dataset is determined by `N`. \n",
    "\n",
    "The accuracy of each model is calculated using hold-out cross-validation. Results are printed in the bottom-right corner of each figure. To get a precise measurement of the model's accuracy, we pick a large `N` and a large `hold-out` fraction. For a more realistic view of what you'd get if you tried to calculate these values yourself, try a smaller `N` with a smaller `holdout` fraction -- `.4` or `.2`. \n",
    "\n",
    "The datapoints that are held out are transparent, while the datapoints used for training are opaque. For the scattered points, color indicates the class label of the datapoint. The background color indicates the output of the classification model at that point in the input space -- blue values correspond to higher probabilities for the black class label and red values to higher probabilities for the white class label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9 Describe the linear classifier's performance. When does it succeed? When does it fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10 Compare the third dataset to the second dataset. Which one does the linear model do better on? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the `MLPClassifier`.\n",
    "\n",
    "#### Q11   What's different about the solutions the neural network finds? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the fitting cell multiple times. Notice that the data, including the hold-out set, doesn't change from run to run (it's being generated pseudo-randomly with a fixed seed).\n",
    "\n",
    "#### Q12 Does the solution found by the neural network differ from run to run? What about that of the linear classifier? Explain. Recall that both are fit via gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q13 Does the neural network ever fail drastically? Why might this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase `holdout` to `0.95` (effectively decreasing the size of the dataset) and again run the fitting cell multiple times.\n",
    "\n",
    "#### Q14 How has the neural network's performance changed? What about the solutions it finds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the `holdout` to `.75`. Uncomment the `KNeighborsClassifier`.\n",
    "\n",
    "#### Q15 Compare and contrast the behavior of the neural network and the nearest-neighbors classifier. Does the nearest-neighbors classifier change from run to run? How is its performance? What do its decision boundaries look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q16 Do the decision boundaries found by the neural network or the nearest-neighbors classifier look closer to the ones you'd draw yourself?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decrease `holdout` to `0.2`, effectively increasing the dataset size.\n",
    "\n",
    "#### Q17 Does this change your answer to the above question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the hyperparameters for the two nonlinear models are exposed so that you can vary them. A hyperparameter is any number that affects the model but that we don't directly learn from the data.\n",
    "\n",
    "The two exposed hyperparameters are `k`, the number of neighbors used by the nearest-neighbors model and the list `hidden_layer_sizes`, listing the sizes of the hidden layers of the neural network (and therefore also how many layers there are). These can interact in interesting ways like the size of the dataset.\n",
    "\n",
    "#### Q18 Do you notice anything else interesting? Write your notes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
